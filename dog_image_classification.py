# -*- coding: utf-8 -*-
"""Dog-image-classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eK2_vR4RcmJE6Oaqo1uyrN5coSzWECJy

**This is classification problem where i'm using Deep neural network models**
"""

import os
import glob
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# from keras.preprocessing.image import  load_img , img_to_array
from tensorflow.keras.preprocessing.image import load_img , ImageDataGenerator , img_to_array

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import cv2
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D,GlobalAveragePooling2D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam

from sklearn.model_selection import train_test_split
from IPython.display import Image, display
import warnings
warnings.filterwarnings('ignore')
import sys

import logging
logger = tf.get_logger()
logger.setLevel(logging.ERROR)

tf.__version__
# sys.version
# !python -V

!pip install opendatasets --quiet

import opendatasets as od

dataset_url = 'https://www.kaggle.com/c/dog-breed-identification/data'

od.download(dataset_url)

main_dir = "../content/dog-breed-identification/"

train_data_dir = main_dir + "train/"
test_data_dir = main_dir + "test/"

len(os.listdir(train_data_dir)), len(os.listdir(test_data_dir))

traindf=pd.read_csv('../content/dog-breed-identification/labels.csv',dtype=str)
testdf=pd.read_csv("../content/dog-breed-identification/sample_submission.csv",dtype=str)

len(traindf) , len(testdf)

df=pd.read_csv('../content/dog-breed-identification/labels.csv')
df.head()

df['breed'].unique()

# Y = pd.get_dummies(df['breed'])
# Y.head()

train_data = df.assign(img_path = lambda x : train_data_dir + x['id'] + '.jpg')
train_data.head()

train_data['img_path'][0]

for i in range(0 , 5):
  display(Image(train_data['img_path'][i]))

# x = np.array([img_to_array(load_img(img,target_size = (128,128))) for img in train_data['img_path'].values.tolist()])
# print(x.shape)
# y = pd.get_dummies(train_data['breed'])
# print(y.shape)

labels_df = pd.read_csv('../content/dog-breed-identification/labels.csv')
labels_df['id'] = labels_df['id'] + '.jpg'
labels_df.breed.value_counts().plot.bar(figsize=(25, 5));



"""#loading the images in images generator

"""

datagen = ImageDataGenerator(rescale=1.0 / 255,
                             rotation_range= 0.2,
                             horizontal_flip=True,
                             zoom_range = 0.2,
                             vertical_flip=True,
                             shear_range=0.2,
                             validation_split=0.2)

train = datagen.flow_from_dataframe(labels_df,
                                    directory=train_data_dir,
                                    x_col='id',
                                    y_col='breed',
                                    subset='training',
                                    target_size=(224, 224),
                                    batch_size=32,
                                    seed=123)

val = datagen.flow_from_dataframe(labels_df,
                                  directory=train_data_dir,
                                  x_col='id',
                                  y_col='breed',
                                  subset='validation',
                                  target_size=(224, 224),
                                  batch_size=32,
                                  seed=123)

# from keras.applications.densenet import DenseNet169
# from keras.applications.resnet import ResNet50

# base = DenseNet169(weights = 'imagenet', include_top = False, input_shape = (224, 224, 3))
# tf.keras.backend.clear_session()

# for layer in base.layers:
#     layer.trainable =  False

# model = tf.keras.Sequential([
#                           base,
#                           # layers.Flatten(input_shape=(128, 128 , 3)),
#                           layers.GlobalMaxPooling2D(),
#                           layers.BatchNormalization(),
#                           layers.Dense(256, activation='relu'),
#                           layers.Dropout(0.5),
#                           layers.BatchNormalization(),
#                           layers.Dense(128, activation='relu'),
#                           layers.Dropout(0.5),
#                           layers.Dense(120, activation='softmax'),
# ])
# model = Sequential([
#   layers.experimental.preprocessing.Rescaling(1./255, input_shape=(180, 180, 3)),
#   layers.Conv2D(16, 3, padding='same', activation='relu'),
#   layers.MaxPooling2D(),
#   layers.Conv2D(32, 3, padding='same', activation='relu'),
#   layers.MaxPooling2D(),
#   layers.Conv2D(64, 3, padding='same', activation='relu'),
#   layers.MaxPooling2D(),
#   layers.Flatten(),
#   layers.Dense(256, activation='relu'),
#   layers.Conv2D(64, 3, padding='same', activation='relu'),
#   layers.MaxPooling2D(),
#   layers.BatchNormalization(),
#   layers.Dropout(0.5),
#   layers.Dense(128, activation='relu'),
#   layers.Dense(120)
# ])

# model = Sequential([
#   layers.experimental.preprocessing.Rescaling(1./255, input_shape=(180, 180, 3)),
#   layers.Conv2D(32, 3, padding='same', activation='relu'),
#   layers.MaxPooling2D(),
#   layers.Conv2D(64, 3, padding='same', activation='relu'),
#   layers.MaxPooling2D(),
#   layers.Conv2D(128, 3, padding='same', activation='relu'),
#   layers.MaxPooling2D(),
#   layers.Flatten(),
#   layers.Dense(256, activation='relu'),
#   layers.Dense(128 ,activation='relu'),
#   layers.Dense(120)
# ])

from keras.applications.inception_resnet_v2 import InceptionResNetV2
from keras.applications.inception_v3 import InceptionV3

# base = InceptionV3(weights = 'imagenet', include_top = False, input_shape = (224, 224, 3))
base = InceptionResNetV2(weights = 'imagenet', include_top = False, input_shape = (224, 224, 3))
tf.keras.backend.clear_session()

for layer in base.layers:
    layer.trainable =  False

model = tf.keras.Sequential([
                          base,
                          # layers.Flatten(input_shape=(224, 224 , 3)),
                          layers.GlobalMaxPooling2D(),
                          layers.BatchNormalization(),
                          layers.Dense(256, activation='relu'),
                          layers.Dropout(0.5),
                          layers.BatchNormalization(),
                          layers.Dense(128, activation='relu'),
                          layers.Dropout(0.5),
                          layers.Dense(120, activation='softmax'),
])

model.compile(loss='categorical_crossentropy' , optimizer = "adam" , metrics=['accuracy'])

epochs = 10

"""##This is the architecture of inceptionResnet model"""

history = model.fit(train,validation_data=val,epochs=2)

train[0][0][0]

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(2)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

# from sklearn.metrics import confusion_matrix

# y_pred = model.predict(test)
# y_pred = np.argmax(y_pred, axis=1)
# y_test = np.argmax(y_test, axis=1)

# cm = confusion_matrix(y_test, y_pred)



!pip install lime

# Commented out IPython magic to ensure Python compatibility.
# %load_ext autoreload
# %autoreload 2
import os,sys
try:
    import lime
except:
    sys.path.append(os.path.join('..', '..')) # add the current directory
    import lime
from lime import lime_image

explainer = lime_image.LimeImageExplainer()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Hide color is the color for a superpixel turned OFF. Alternatively, if it is NONE, the superpixel will be replaced by the average of its pixels
# explanation = explainer.explain_instance(train[0][0][1].astype('double'), model.predict, top_labels=5, hide_color=0, num_samples=1000)

plt.imshow(train[0][0][1])

from skimage.segmentation import mark_boundaries

temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=5, hide_rest=False)
plt.imshow(mark_boundaries(temp , mask))

